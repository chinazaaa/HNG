{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "_Mo2vxkP-0JK",
    "outputId": "85d4c99f-8b12-41bb-faf7-6a0a3f1fb237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Processing DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "nmBbzycb_7O-",
    "outputId": "7aef17ea-cf7f-46ea-9af8-0c0efddb24fb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape is (50, 12288)\n",
      "X.shape is (54, 12288)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fc48bac0a560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mX_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0my_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import os, cv2, itertools \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "PLATE_NO_DIR = './plate_number/'\n",
    "NEG_IMG_DIR = './negative_images/'\n",
    "\n",
    "ROWS = 64\n",
    "COLS = 64\n",
    "CHANNELS = 3\n",
    "\n",
    "plate_no_images = [PLATE_NO_DIR + i for i in os.listdir(PLATE_NO_DIR)]\n",
    "neg_img = [NEG_IMG_DIR + i for i in os.listdir(NEG_IMG_DIR)]\n",
    "\n",
    "\n",
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "def prep_data(images, label=None):\n",
    "    m = len(images)\n",
    "    n_x = ROWS*COLS*CHANNELS\n",
    "  \n",
    "    X = np.ndarray((m,n_x), dtype = np.uint8)\n",
    "    y = np.zeros((m,1))\n",
    "    print(\"X.shape is {}\".format(X.shape))\n",
    "  \n",
    "    for i,image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        X[i,:] = image.reshape(1,-1)\n",
    "        y[i,:] = label\n",
    "\n",
    "\n",
    "    return X,y\n",
    "\n",
    "X1,y1 = prep_data(plate_no_images, label=1)\n",
    "X2,y2 = prep_data(neg_img[10:],label=0)\n",
    "X_img = np.r_[X1,X2]\n",
    "y_img = np.r_[y1,y2]\n",
    "data = np.c_[X,y]\n",
    "\n",
    "\n",
    "\n",
    "def show_images(X, y, idx) :\n",
    "    classes = {0: 'Normal Image',\n",
    "           1: 'License Plate'}\n",
    "    \n",
    "    image = X[idx]\n",
    "    image = image.reshape((ROWS, COLS, CHANNELS))\n",
    "    plt.figure(figsize=(4,2))\n",
    "    plt.imshow(image)\n",
    "    plt.title(classes[y[idx][0]])\n",
    "    plt.show()\n",
    "  \n",
    "\n",
    "show_images(X_img, y_img, 1)\n",
    "# str(classes[y[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cfT5L_mMAOSm",
    "outputId": "82fde8e9-a30a-4992-9cd2-4659fb236b61"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "clf = LogisticRegressionCV()\n",
    "\n",
    "# X_img_lr, y_img_lr = X_img.T, y_img.T.ravel()\n",
    "\n",
    "clf.fit(X_img,y_img)\n",
    "\n",
    "print(\"Model accuracy: {:.2f}%\".format(clf.score(X_img, y_img)*100))\n",
    "\n",
    "def show_image_prediction(X, idx, model):\n",
    "    image = X[idx].reshape(1, -1)\n",
    "    image_class = classes[model.predict(image).item()]\n",
    "    image = image.reshape((ROWS, COLS, CHANNELS))\n",
    "    plt.figure(figsize = (4,2))\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Test {} : I think this is a {}\".format(idx, image_class))\n",
    "    plt.show()\n",
    "  \n",
    "for i in np.random.randint(0, len(X_img), 10) :\n",
    "      show_image_prediction(X_img, i, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN Model Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as tch\n",
    "from torchvision import models, datasets, transforms\n",
    "from collections import OrderedDict\n",
    "from torch import nn, optim\n",
    "import shutil\n",
    "import random\n",
    "# import helper\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,precision_score,f1_score,roc_curve,roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(images, label=None):\n",
    "    m = len(images)\n",
    "    n_x = ROWS*COLS*CHANNELS\n",
    "  \n",
    "    X = np.empty((0,224,224,3))\n",
    "    y = np.empty((0,1))\n",
    "    \n",
    "    trans = transforms.Compose([  transforms.ToTensor(),\n",
    "                                  transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                       [0.229, 0.224, 0.225])])\n",
    "    print(\"X.shape is {}\".format(X.shape))\n",
    "\n",
    "    for i,image_file in enumerate(images):\n",
    "#         image = read_image(image_file)\n",
    "#         X[i,:] = image.reshape(1,-1)\n",
    "#         y[i,:] = label\n",
    "        img = read_image(image_file)\n",
    "        img = trans(img).numpy().transpose((1, 2, 0))\n",
    "    \n",
    "        #concate images together as flatten\n",
    "        out = np.array([[label]]) #output\n",
    "        con_img = np.expand_dims(img,0)\n",
    "#         print(image_file)\n",
    "#         print(con_img.shape)\n",
    "\n",
    "        img_d = np.copy(con_img)\n",
    "\n",
    "        X = np.r_[X,img_d]\n",
    "        y = np.r_[y,out]\n",
    "    return X,y\n",
    "ROWS = 224\n",
    "COLS = 224\n",
    "CHANNELS = 3\n",
    "X1,y1 = prep_data(plate_no_images, label=1)\n",
    "X2,y2 = prep_data(neg_img,label=0)\n",
    "X = np.r_[X1,X2]\n",
    "y = np.r_[y1,y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encod = OneHotEncoder()\n",
    "y = encod.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting dataset into train,remaining\n",
    "split = StratifiedShuffleSplit(test_size=0.2)\n",
    "\n",
    "for trn_idx, tst_idx in split.split(X,y_):\n",
    "    x_trn, x_tst = X[trn_idx], X[tst_idx]\n",
    "    y_trn, y_tst = y[trn_idx], y[tst_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /home/vincycode/.cache/torch/checkpoints/alexnet-owt-4df8aa71.pth\n",
      " 25%|██▍       | 60522496/244418560 [38:01<3:19:34, 15357.13it/s]"
     ]
    }
   ],
   "source": [
    "res = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = tch.load('fcmodel_train.pt')\n",
    "# fc = mod['model_arc']\n",
    "# fc.load_state_dict(mod['state_dict'])\n",
    "\n",
    "resnet = torch.load('cnnmodel_beta2.pt')\n",
    "cnn = resnet['model_arc']\n",
    "cnn.load_state_dict(resnet['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_info = {'model_arc'  : res,\n",
    "                  'state_dict' : res.state_dict()}\n",
    "torch.save(cnn_model_info, 'cnnmodel_beta2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer1 = optim.SGD([{'params' : cnn.parameters()},\n",
    "                        {'params' : fc.parameters()}], \n",
    "                        lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0\n",
    "for child in cnn.children():\n",
    "  ct += 1\n",
    "  if ct < 8:\n",
    "      for param in child.parameters():\n",
    "          param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_model:\n",
    "  \n",
    "  def __init__(self,cnn = None,model = None, lr=0.01):\n",
    "    self.cnn = cnn\n",
    "    self.model = model\n",
    "    self.criterion = None\n",
    "    self.optimizer = None\n",
    "    self.weights = None\n",
    "    self.y_scores_train = []\n",
    "    self.y_scores_test = []\n",
    "   # self.valid_loader = te\n",
    "   # self.train_loader = testset\n",
    "    #self.batch = batch\n",
    "    #self.epochs = epochs\n",
    "    self.train_loss = []\n",
    "    self.test_loss = []\n",
    "    self.train_acc = []\n",
    "    self.test_acc = []\n",
    "    self.train_rec = []\n",
    "    self.test_rec = []\n",
    "    self.train_f1_scr = []\n",
    "    self.test_f1_scr = []\n",
    "    self.train_pre = []\n",
    "    self.test_pre = []\n",
    "    self.t_epch = 0\n",
    "    #self.save_on_train = save_on_train\n",
    "    \n",
    "  def train(self,train_loader=None, valid_loader=None,criterion=None,optimizer=None,epochs=50,batch_size=30):\n",
    "    \n",
    "    model = self.model\n",
    "    cnn = self.cnn\n",
    "    valid_rec_min = 0.0\n",
    "    train_on_gpu = True #tch.cuda.is_available()\n",
    "    new_state = model.state_dict()\n",
    "    self.t_epch += epochs\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "      # keep track of training and validation loss and scores\n",
    "        train_loss    = 0.0\n",
    "        test_loss    = 0.0\n",
    "        train_acc     = 0.0\n",
    "        train_pre     = 0.0\n",
    "        train_rec     = 0.0\n",
    "        train_f1_scr  = 0.0\n",
    "        test_acc      = 0.0\n",
    "        test_pre      = 0.0\n",
    "        test_rec      = 0.0\n",
    "        test_f1_scr   = 0.0\n",
    "      \n",
    "      ###################\n",
    "      # train the model #\n",
    "      ###################\n",
    "        model.cuda()\n",
    "        for data, target in train_loader:\n",
    "       \n",
    "          # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu: \n",
    "                cnn.eval()\n",
    "                model.train()\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            else:\n",
    "                cnn.eval()\n",
    "                model.cpu()\n",
    "                model.train()\n",
    "                data, target = data.cpu(), target.cpu()\n",
    "     \n",
    "          # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "     \n",
    "            data = data.cpu()\n",
    "            img = data[:,:,:,:3]\n",
    "            img2 = data[:,:,:,3:]\n",
    "            img = cnn(tch.from_numpy(img.numpy().transpose((0, 3, 1, 2))))\n",
    "            img2 = cnn(tch.from_numpy(img2.numpy().transpose((0, 3, 1, 2))))\n",
    "\n",
    "            data = tch.cat((img,img2),1).cuda()\n",
    "          \n",
    "          \n",
    "          \n",
    "          # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "#             print(f'output is {output} and target is {target}')\n",
    "          # calculate the batch loss\n",
    "            loss = criterion(output, target)                   \n",
    "      \n",
    "          # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "      \n",
    "          # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "       \n",
    "          # update training loss\n",
    "            train_loss     +=  loss.item()*data.size(0)\n",
    "            train_acc      +=  accuracy_score(target.cpu()[:,1:], output.cpu().topk(1)[1])\n",
    "            train_pre      +=  precision_score(target.cpu()[:,1:], output.cpu().topk(1)[1])\n",
    "            train_rec      +=  recall_score(target.cpu()[:,1:], output.cpu().topk(1)[1])\n",
    "            train_f1_scr   +=  f1_score(target.cpu()[:,1:], output.cpu().topk(1)[1])\n",
    "  \n",
    "        \n",
    "      ######################    \n",
    "      # validate the model #\n",
    "      ######################\n",
    "        cnn.eval()\n",
    "        model.eval()\n",
    "     \n",
    "        for data, target in valid_loader:\n",
    "      \n",
    "          # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                data = data.cpu()\n",
    "                img = data[:,:,:,:3]\n",
    "                img2 = data[:,:,:,3:]\n",
    "                img = cnn(tch.from_numpy(img.numpy().transpose((0, 3, 1, 2))))\n",
    "                img2 = cnn(tch.from_numpy(img2.numpy().transpose((0, 3, 1, 2))))\n",
    "\n",
    "                data = tch.cat((img,img2),1).cuda()\n",
    "       \n",
    "          # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "          \n",
    "          # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "                    \n",
    "          # update average validation loss \n",
    "            test_loss     += loss.item()*data.size(0)\n",
    "            test_acc       += accuracy_score(target.cpu()[:,1:], output.cpu().topk(1)[1])\n",
    "            test_pre       += precision_score(target.cpu()[:,1:], output.cpu().topk(1)[1])\n",
    "            test_rec       +=  recall_score(target.cpu()[:,1:], output.cpu().topk(1)[1])\n",
    "            test_f1_scr    +=  f1_score(target.cpu()[:,1:], output.cpu().topk(1)[1])\n",
    "            \n",
    "                \n",
    "      # calculate average losses and scores\n",
    "        train_loss = train_loss/round(len(train_loader.dataset)/batch_size)\n",
    "        test_loss = test_loss/round(len(valid_loader.dataset)/batch_size)\n",
    "      \n",
    "        train_acc       =   1 if  train_acc/round(len(train_loader.dataset)/batch_size) > 1 else train_acc/round(len(train_loader.dataset)/batch_size)\n",
    "\n",
    "        train_pre       =   1 if train_pre/round(len(train_loader.dataset)/batch_size) > 1 else train_pre/round(len(train_loader.dataset)/batch_size)\n",
    "\n",
    "        train_rec       =   1 if train_rec/round(len(train_loader.dataset)/batch_size) > 1 else train_rec/round(len(train_loader.dataset)/batch_size)\n",
    "                                     \n",
    "        train_f1_scr    =   1 if train_f1_scr/round(len(train_loader.dataset)/batch_size) > 1 else train_f1_scr/round(len(train_loader.dataset)/batch_size)\n",
    "\n",
    "        test_acc        =   1 if test_acc/round(len(valid_loader.dataset)/batch_size) > 1 else test_acc/round(len(valid_loader.dataset)/batch_size)\n",
    "\n",
    "        test_pre        =   1 if test_pre/round(len(valid_loader.dataset)/batch_size) > 1 else test_pre/round(len(valid_loader.dataset)/batch_size)\n",
    "  \n",
    "        test_rec        =   1 if test_rec/round(len(valid_loader.dataset)/batch_size,5) > 1 else test_rec/round(len(valid_loader.dataset)/batch_size,5)\n",
    "\n",
    "        test_f1_scr    =   1 if test_f1_scr/round(len(valid_loader.dataset)/batch_size,5) > 1 else test_f1_scr/round(len(valid_loader.dataset)/batch_size,5) \n",
    "\n",
    "      \n",
    "\n",
    "            \n",
    "      #append train scores\n",
    "        self.train_loss.append(train_loss)\n",
    "        self.train_acc.append(train_acc)\n",
    "        self.train_rec.append(train_rec)\n",
    "        self.train_f1_scr.append(train_f1_scr)\n",
    "        self.train_pre.append(train_pre)\n",
    "        \n",
    "      #append test scores\n",
    "        self.test_loss.append(test_loss)\n",
    "        self.test_acc.append(test_acc)\n",
    "        self.test_rec.append(test_rec)\n",
    "        self.test_f1_scr.append(test_f1_scr)\n",
    "        self.test_pre.append(test_pre)\n",
    "        \n",
    "      # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} '.format(epoch, train_loss, test_loss))\n",
    "      \n",
    "        print('train Accuracy score : {:.6f} \\ttrain Precision Score: {:.6f} \\ttrain Recall Score: {:.6f}\\ttrain f1 Score: {:.6f}'.format(train_acc, train_pre, train_rec, train_f1_scr))\n",
    "      \n",
    "        print('test Accuracy score : {:.6f} \\ttest Precision Score: {:.6f} \\ttest Recall Score: {:.6f}\\ttest f1 Score: {:.6f}'.format(test_acc, test_pre, test_rec, test_f1_scr))\n",
    "        self.model = model\n",
    "        self.cnn = cnn\n",
    "        self.save_model()\n",
    "      # save model if validation loss has decreased\n",
    "        if test_rec > valid_rec_min:\n",
    "            print('test recall increased ({:.6f} --> {:.6f}).  Saving model ...\\n\\n'.format(valid_rec_min,test_rec))\n",
    "            #self.model = model \n",
    "            #self.cnn = cnn\n",
    "            \n",
    "            #Save State_dict\n",
    "            fc_model_info = {'model_arc'  : model,\n",
    "                             'state_dict' : model.state_dict()}\n",
    "            \n",
    "            cnn_model_info = {'model_arc'  : cnn,\n",
    "                              'state_dict' : cnn.state_dict()}\n",
    "\n",
    "            torch.save(fc_model_info, 'fcmodel_beta2.pt')\n",
    "            torch.save(cnn_model_info, 'cnnmodel_beta2.pt')\n",
    "            valid_rec_min = test_rec\n",
    "\n",
    "  def predict(self,data):\n",
    "    model = self.model\n",
    "    model.cpu().eval()\n",
    "    cnn = self.cnn\n",
    "    \n",
    "    cnn.cpu().eval()\n",
    "    data = data.cpu()\n",
    "    img = data[:,:,:,:3]\n",
    "    img2 = data[:,:,:,3:]\n",
    "      \n",
    "    img = cnn(tch.from_numpy(img.numpy().transpose((0, 3, 1, 2))))\n",
    "    img2 = cnn(tch.from_numpy(img2.numpy().transpose((0, 3, 1, 2))))\n",
    "\n",
    "    data = tch.cat((img,img2),1)\n",
    "\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    \n",
    "    return output.cpu().topk(1)[1].detach().numpy()\n",
    "  \n",
    "  \n",
    "  def save_model(self):\n",
    "    model = self.model\n",
    "    cnn = self.cnn\n",
    "\n",
    "      #Save State_dict\n",
    "    fc_model_info = {'model_arc'  : model,\n",
    "                     'state_dict' : model.state_dict()}\n",
    "\n",
    "    cnn_model_info = {'model_arc'  : cnn,\n",
    "                      'state_dict' : cnn.state_dict()}\n",
    "\n",
    "    torch.save(fc_model_info, 'fcmodel_train2.pt')\n",
    "    torch.save(cnn_model_info, 'cnnmodel_train2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(cnn=cnn, model=fc)\n",
    "model.train(train_loader=train_loader,valid_loader=val_loader,criterion=criterion,optimizer=optimizer1,epochs=15)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ML_Task_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
